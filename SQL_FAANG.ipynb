{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO2kVjXNCsU+9gDxbTykFLd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankitarm/SQL_Data_Engineer/blob/main/SQL_FAANG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1. Meta/Facebook (Hard Level)\n",
        "\n",
        "---\n",
        "\n",
        "**Find the famous percentage of each user.**\n",
        "\n",
        "> *Famous Percentage = number of followers a user has / total number of users on the platform*\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“˜ Schema and Dataset\n",
        "\n",
        "**`famous` Table**\n",
        "\n",
        "| user\\_id | follower\\_id |\n",
        "| -------- | ------------ |\n",
        "| 1        | 2            |\n",
        "| 1        | 3            |\n",
        "| 2        | 4            |\n",
        "| 5        | 1            |\n",
        "| 5        | 3            |\n",
        "| 11       | 7            |\n",
        "| 12       | 8            |\n",
        "| 13       | 5            |\n",
        "| 13       | 10           |\n",
        "| 14       | 12           |\n",
        "| 14       | 3            |\n",
        "| 15       | 14           |\n",
        "| 15       | 13           |\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  Explanation of the Query\n",
        "\n",
        "1. **`distinct_users` CTE:**\n",
        "   Uses `UNION` on `user_id` and `follower_id` to find the total unique users on the platform.\n",
        "\n",
        "2. **`follower_count` CTE:**\n",
        "   Groups by `user_id` and counts how many followers each user has.\n",
        "\n",
        "3. **Final SELECT:**\n",
        "   Joins the total user count with follower counts to calculate\n",
        "   `famous_percentage = follower_count / total_users`.\n"
      ],
      "metadata": {
        "id": "4hy51uzdZEXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```sql\n",
        "/*\n",
        "CREATE TABLE famous (user_id INT, follower_id INT);\n",
        "INSERT INTO famous VALUES\n",
        "(1, 2), (1, 3), (2, 4), (5, 1), (5, 3),\n",
        "(11, 7), (12, 8), (13, 5), (13, 10),\n",
        "(14, 12), (14, 3), (15, 14), (15, 13);\n",
        "\n",
        "select * from famous;\n",
        "*/\n",
        "\n",
        "WITH USERFOLTBL AS (\n",
        "  select user_id AS ID from famous\n",
        "  UNION\n",
        "  SELECT follower_id AS ID FROM famous)\n",
        "SELECT user_id,\n",
        "ROUND((COUNT(DISTINCT follower_id)*1.0/COUNT(DISTINCT ID ))*100,2) AS PERCENTAGE\n",
        "FROM famous, USERFOLTBL  \n",
        "GROUP BY user_id\n",
        "\n"
      ],
      "metadata": {
        "id": "f4p7jg_jFxft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2. LinkedIn (Hard Level)\n",
        "\n",
        "---\n",
        "\n",
        "**Find how many users had Google as their next employer immediately after Microsoft (no employers in between).**\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“˜ Schema and Dataset\n",
        "\n",
        "**`linkedin_users` Table**\n",
        "\n",
        "| user\\_id | employer  | position         | start\\_date | end\\_date  |\n",
        "| -------- | --------- | ---------------- | ----------- | ---------- |\n",
        "| 1        | Microsoft | developer        | 2020-04-13  | 2021-11-01 |\n",
        "| 1        | Google    | developer        | 2021-11-01  | NULL       |\n",
        "| 2        | Google    | manager          | 2021-01-01  | 2021-01-11 |\n",
        "| 2        | Microsoft | manager          | 2021-01-11  | NULL       |\n",
        "| 3        | Microsoft | analyst          | 2019-03-15  | 2020-07-24 |\n",
        "| 3        | Amazon    | analyst          | 2020-08-01  | 2020-11-01 |\n",
        "| 3        | Google    | senior analyst   | 2020-11-01  | 2021-03-04 |\n",
        "| 4        | Google    | junior developer | 2018-06-01  | 2021-11-01 |\n",
        "| 4        | Google    | senior developer | 2021-11-01  | NULL       |\n",
        "| 5        | Microsoft | manager          | 2017-09-26  | NULL       |\n",
        "| 6        | Google    | CEO              | 2015-10-02  | NULL       |\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  Explanation of the Query\n",
        "\n",
        "1. **CTE with `LEAD()` Function:**\n",
        "   For each user, sort jobs by `start_date` and use `LEAD()` to fetch the **next employer** and **next start\\_date** after the current row.\n",
        "\n",
        "2. **Filter Microsoft Records:**\n",
        "   In the CTE, focus on rows where the **current employer is Microsoft** and **next employer is Google**.\n",
        "\n",
        "3. **Final SELECT:**\n",
        "   Count how many users had Google immediately after Microsoft (without a job in between).\n",
        "\n",
        "```\n",
        "\n",
        "ð’ðœð¡ðžð¦ðš ðšð§ð ðƒðšð­ðšð¬ðžð­:\n",
        "CREATE TABLE linkedin_users (user_id INT,employer VARCHAR(255),position VARCHAR(255),start_date DATETIME,end_date DATETIME);\n",
        "\n",
        "INSERT INTO linkedin_users (user_id, employer, position, start_date, end_date) VALUES(1, 'Microsoft', 'developer', '2020-04-13', '2021-11-01'),(1, 'Google', 'developer', '2021-11-01', NULL),(2, 'Google', 'manager', '2021-01-01', '2021-01-11'),(2, 'Microsoft', 'manager', '2021-01-11', NULL),(3, 'Microsoft', 'analyst', '2019-03-15', '2020-07-24'),(3, 'Amazon', 'analyst', '2020-08-01', '2020-11-01'),(3, 'Google', 'senior analyst', '2020-11-01', '2021-03-04'),(4, 'Google', 'junior developer', '2018-06-01', '2021-11-01'),(4, 'Google', 'senior developer', '2021-11-01', NULL),(5, 'Microsoft', 'manager', '2017-09-26', NULL),(6, 'Google', 'CEO', '2015-10-02', NULL);\n",
        "```\n",
        "-----------"
      ],
      "metadata": {
        "id": "NdpMfzd9S9ax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```sql\n",
        "\n",
        "\n",
        "WITH NXTEMPTBL AS(\n",
        "\tSELECT user_id,employer,\n",
        "\tLEAD(employer) OVER(PARTITION BY user_id ORDER BY start_date) AS NXTEMP\n",
        "\tFROM linkedin_users)\n",
        "SELECT user_id, employer,NXTEMP  FROM NXTEMPTBL\n",
        "WHERE employer = 'Microsoft' AND NXTEMP = 'Google'\n",
        "\n",
        "Output:\n",
        "\n",
        "1\tMicrosoft\tGoogle\n",
        "```"
      ],
      "metadata": {
        "id": "v4cnBchjP01x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3. American Express (Medium Level)\n",
        "\n",
        "---\n",
        "\n",
        "**Find the customer with the third highest total transaction amount. Output the customerâ€™s id, first name, and last name. Use ranking with no gaps.**\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“˜ Schema and Dataset\n",
        "\n",
        "**`customers` Table**\n",
        "\n",
        "| id | first\\_name | last\\_name | city        | address      | phone\\_number |\n",
        "| -- | ----------- | ---------- | ----------- | ------------ | ------------- |\n",
        "| 1  | Jill        | Doe        | New York    | 123 Main St  | 555-1234      |\n",
        "| 2  | Henry       | Smith      | Los Angeles | 456 Oak Ave  | 555-5678      |\n",
        "| 3  | William     | Johnson    | Chicago     | 789 Pine Rd  | 555-8765      |\n",
        "| 4  | Emma        | Daniel     | Houston     | 321 Maple Dr | 555-4321      |\n",
        "| 5  | Charlie     | Davis      | Phoenix     | 654 Elm St   | 555-6789      |\n",
        "\n",
        "**`card_orders` Table**\n",
        "\n",
        "| order\\_id | cust\\_id | order\\_date         | order\\_details | total\\_order\\_cost |\n",
        "| --------- | -------- | ------------------- | -------------- | ------------------ |\n",
        "| 1         | 1        | 2024-11-01 10:00:00 | Electronics    | 200                |\n",
        "| 2         | 2        | 2024-11-02 11:30:00 | Groceries      | 150                |\n",
        "| 3         | 1        | 2024-11-03 15:45:00 | Clothing       | 120                |\n",
        "| 4         | 3        | 2024-11-04 09:10:00 | Books          | 90                 |\n",
        "| 8         | 3        | 2024-11-08 10:20:00 | Groceries      | 130                |\n",
        "| 9         | 1        | 2024-11-09 12:00:00 | Books          | 180                |\n",
        "| 10        | 4        | 2024-11-10 11:15:00 | Electronics    | 200                |\n",
        "| 11        | 5        | 2024-11-11 14:45:00 | Furniture      | 150                |\n",
        "| 12        | 2        | 2024-11-12 09:30:00 | Furniture      | 180                |\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  Explanation of the Query\n",
        "\n",
        "1. **CustomerTransactionTotals CTE:**\n",
        "   Sum the `total_order_cost` for each `cust_id` from `card_orders`.\n",
        "\n",
        "2. **RankedTransactions CTE:**\n",
        "   Use `DENSE_RANK()` to rank customers by their total transaction amount in descending order (no gaps in rank).\n",
        "\n",
        "3. **Final Selection:**\n",
        "   Join the 3rd ranked customer with the `customers` table and retrieve their `id`, `first_name`, and `last_name`.\n",
        "\n",
        "```sql\n",
        "\n",
        "ð’ðœð¡ðžð¦ðš ðšð§ð ðƒðšð­ðšð¬ðžð­:\n",
        "CREATE TABLE customers (id INT,first_name VARCHAR(50),last_name VARCHAR(50),city VARCHAR(100),address VARCHAR(200),phone_number VARCHAR(20));\n",
        "\n",
        "INSERT INTO customers (id, first_name, last_name, city, address, phone_number) VALUES(1, 'Jill', 'Doe', 'New York', '123 Main St', '555-1234'),(2, 'Henry', 'Smith', 'Los Angeles', '456 Oak Ave', '555-5678'),(3, 'William', 'Johnson', 'Chicago', '789 Pine Rd', '555-8765'),(4, 'Emma', 'Daniel', 'Houston', '321 Maple Dr', '555-4321'),(5, 'Charlie', 'Davis', 'Phoenix', '654 Elm St', '555-6789');\n",
        "\n",
        "CREATE TABLE card_orders (order_id INT,cust_id INT,order_date DATETIME,order_details VARCHAR(255),total_order_cost INT);\n",
        "\n",
        "INSERT INTO card_orders (order_id, cust_id, order_date, order_details, total_order_cost) VALUES(1, 1, '2024-11-01 10:00:00', 'Electronics', 200),(2, 2, '2024-11-02 11:30:00', 'Groceries', 150),(3, 1, '2024-11-03 15:45:00', 'Clothing', 120),(4, 3, '2024-11-04 09:10:00', 'Books', 90),(8, 3, '2024-11-08 10:20:00', 'Groceries', 130),(9, 1, '2024-11-09 12:00:00', 'Books', 180),(10, 4, '2024-11-10 11:15:00', 'Electronics', 200),(11, 5, '2024-11-11 14:45:00', 'Furniture', 150),(12, 2, '2024-11-12 09:30:00', 'Furniture', 180);\n",
        "```"
      ],
      "metadata": {
        "id": "f3PLBvKITd3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```sql\n",
        "\n",
        "sOLUTION 1:\n",
        "\n",
        "WITH HIGHTRAN AS (\n",
        "  SELECT cust_id,SUM(total_order_cost) AS TOTAL_AMOUNT\n",
        "  FROM card_orders\n",
        "  GROUP BY cust_id\n",
        "  ORDER BY TOTAL_AMOUNT DESC\n",
        "  LIMIT 1 OFFSET 2)\n",
        "SELECT cust_id, first_name, last_name\n",
        "FROM HIGHTRAN HT LEFT JOIN  customers C\n",
        "ON C.id = HT.cust_id\n",
        "\n",
        "\n",
        "SOLUTION 2:\n",
        "\n",
        "WITH CustomerTotals AS (\n",
        "    SELECT\n",
        "        cust_id,\n",
        "        SUM(total_order_cost) AS total_amount\n",
        "    FROM card_orders\n",
        "    GROUP BY cust_id\n",
        "),\n",
        "RankedCustomers AS (\n",
        "    SELECT\n",
        "        cust_id,\n",
        "        total_amount,\n",
        "        DENSE_RANK() OVER (ORDER BY total_amount DESC) AS rnk\n",
        "    FROM CustomerTotals\n",
        ")\n",
        "SELECT\n",
        "    c.id AS cust_id,\n",
        "    c.first_name,\n",
        "    c.last_name\n",
        "FROM RankedCustomers r\n",
        "JOIN customers c ON r.cust_id = c.id\n",
        "WHERE r.rnk = 3;\n",
        "\n",
        "\n",
        "| Requirement                              | Use                        |\n",
        "| ---------------------------------------- | -------------------------- |\n",
        "| Just the 3rd highest row (ignoring ties) | `LIMIT 1 OFFSET 2`         |\n",
        "| All customers tied at 3rd highest amount | `RANK()` or `DENSE_RANK()` |\n",
        "| Need clarity, flexibility, and accuracy  | `RANK()`/`DENSE_RANK()`    |\n"
      ],
      "metadata": {
        "id": "F9jbIG8KULhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Q4. Amazon (Hard Level)\n",
        "\n",
        "Amazon wants to identify products that are **exclusive to their platform** and not sold on **Top Shop** or **Macy's**.\n",
        "\n",
        "Two products are considered equal if they share the **same product name** and the **same maximum retail price (mrp column)**.\n",
        "\n",
        "Your task is to find such exclusive Amazon products and output the following details:\n",
        "\n",
        "* product name\n",
        "* brand name\n",
        "* price\n",
        "* rating\n",
        "\n",
        "### ð’ðœð¡ðžð¦ðš ðšð§ð ðƒðšð­ðšð¬ðžð­ (Table Format)\n",
        "\n",
        "#### Table: `innerwear_amazon_com`\n",
        "\n",
        "| product\\_name | mrp | price | pdp\\_url | brand\\_name | product\\_category | retailer | description  | rating | review\\_count | style\\_attributes | total\\_sizes | available\\_size | color |\n",
        "| ------------- | --- | ----- | -------- | ----------- | ----------------- | -------- | ------------ | -----: | ------------: | ----------------- | ------------ | --------------- | ----- |\n",
        "| ProductA      | 100 | 80    | url1     | BrandA      | Category1         | Amazon   | DescriptionA |    4.5 |           100 | StyleA            | M,L          | M               | Red   |\n",
        "| ProductB      | 200 | 180   | url2     | BrandB      | Category1         | Amazon   | DescriptionB |    4.2 |           150 | StyleB            | S,M,L        | S               | Blue  |\n",
        "| ProductC      | 300 | 250   | url3     | BrandC      | Category2         | Amazon   | DescriptionC |    4.8 |           200 | StyleC            | L,XL         | L               | Green |\n",
        "\n",
        "#### Table: `innerwear_macys_com`\n",
        "\n",
        "| product\\_name | mrp | price | pdp\\_url | brand\\_name | product\\_category | retailer | description  | rating | review\\_count | style\\_attributes | total\\_sizes | available\\_size | color  |\n",
        "| ------------- | --- | ----- | -------- | ----------- | ----------------- | -------- | ------------ | -----: | ------------: | ----------------- | ------------ | --------------- | ------ |\n",
        "| ProductA      | 100 | 85    | url4     | BrandA      | Category1         | Macys    | DescriptionA |    4.5 |            90 | StyleA            | M,L          | M               | Red    |\n",
        "| ProductD      | 150 | 130   | url5     | BrandD      | Category3         | Macys    | DescriptionD |    4.0 |            80 | StyleD            | S,M          | S               | Yellow |\n",
        "| ProductE      | 250 | 210   | url6     | BrandE      | Category4         | Macys    | DescriptionE |    3.9 |            60 | StyleE            | M,L          | L               | Black  |\n",
        "\n",
        "#### Table: `innerwear_topshop_com`\n",
        "\n",
        "| product\\_name | mrp | price | pdp\\_url | brand\\_name | product\\_category | retailer | description  | rating | review\\_count | style\\_attributes | total\\_sizes | available\\_size | color  |\n",
        "| ------------- | --- | ----- | -------- | ----------- | ----------------- | -------- | ------------ | -----: | ------------: | ----------------- | ------------ | --------------- | ------ |\n",
        "| ProductB      | 200 | 190   | url7     | BrandB      | Category1         | TopShop  | DescriptionB |    4.1 |            95 | StyleB            | S,M,L        | M               | Blue   |\n",
        "| ProductF      | 100 | 90    | url8     | BrandF      | Category3         | TopShop  | DescriptionF |    3.5 |            50 | StyleF            | XS,S         | S               | Pink   |\n",
        "| ProductG      | 300 | 270   | url9     | BrandG      | Category5         | TopShop  | DescriptionG |    4.3 |            70 | StyleG            | M,L,XL       | M               | Purple |\n",
        "\n",
        "---\n",
        "\n",
        "### ð„ð±ð©ð¥ðšð§ðšð­ð¢ð¨ð§ ð­ð¨ ð’ð¨ð¥ð¯ðž ðð®ðžð«ð²\n",
        "\n",
        "1. **LEFT JOINs**: Join Amazonâ€™s table to Macyâ€™s and TopShop on **both** `product_name` **and** `mrp` to treat products with the same name and MRP as identical across retailers.\n",
        "2. **Filter Exclusives**: Keep only rows where **no match** is found in Macyâ€™s **and** TopShop (i.e., joined columns are `NULL`), meaning the product exists only on Amazon.\n",
        "3. **Select & Sort**: Return `product_name`, `brand_name`, `price`, `rating` from Amazon for those exclusive items (optionally sort by `product_name`).\n",
        "\n",
        "---\n",
        "\n",
        "**Schema and Dataset:**\n",
        "\n",
        "```sql\n",
        "CREATE TABLE innerwear_amazon_com (\n",
        "  product_name VARCHAR(255),\n",
        "  mrp VARCHAR(50),\n",
        "  price VARCHAR(50),\n",
        "  pdp_url VARCHAR(255),\n",
        "  brand_name VARCHAR(100),\n",
        "  product_category VARCHAR(100),\n",
        "  retailer VARCHAR(100),\n",
        "  description VARCHAR(255),\n",
        "  rating FLOAT,\n",
        "  review_count INT,\n",
        "  style_attributes VARCHAR(255),\n",
        "  total_sizes VARCHAR(50),\n",
        "  available_size VARCHAR(50),\n",
        "  color VARCHAR(50)\n",
        ");\n",
        "\n",
        "CREATE TABLE innerwear_macys_com (\n",
        "  product_name VARCHAR(255),\n",
        "  mrp VARCHAR(50),\n",
        "  price VARCHAR(50),\n",
        "  pdp_url VARCHAR(255),\n",
        "  brand_name VARCHAR(100),\n",
        "  product_category VARCHAR(100),\n",
        "  retailer VARCHAR(100),\n",
        "  description VARCHAR(255),\n",
        "  rating FLOAT,\n",
        "  review_count FLOAT,\n",
        "  style_attributes VARCHAR(255),\n",
        "  total_sizes VARCHAR(50),\n",
        "  available_size VARCHAR(50),\n",
        "  color VARCHAR(50)\n",
        ");\n",
        "\n",
        "CREATE TABLE innerwear_topshop_com (\n",
        "  product_name VARCHAR(255),\n",
        "  mrp VARCHAR(50),\n",
        "  price VARCHAR(50),\n",
        "  pdp_url VARCHAR(255),\n",
        "  brand_name VARCHAR(100),\n",
        "  product_category VARCHAR(100),\n",
        "  retailer VARCHAR(100),\n",
        "  description VARCHAR(255),\n",
        "  rating FLOAT,\n",
        "  review_count FLOAT,\n",
        "  style_attributes VARCHAR(255),\n",
        "  total_sizes VARCHAR(50),\n",
        "  available_size VARCHAR(50),\n",
        "  color VARCHAR(50)\n",
        ");\n",
        "\n",
        "\n",
        "INSERT INTO innerwear_topshop_com VALUES\n",
        "('ProductB', '200', '190', 'url7', 'BrandB', 'Category1', 'TopShop', 'DescriptionB', 4.1, 95, 'StyleB', 'S,M,L', 'M', 'Blue'),\n",
        "('ProductF', '100', '90', 'url8', 'BrandF', 'Category3', 'TopShop', 'DescriptionF', 3.5, 50, 'StyleF', 'XS,S', 'S', 'Pink'),\n",
        "('ProductG', '300', '270', 'url9', 'BrandG', 'Category5', 'TopShop', 'DescriptionG', 4.3, 70, 'StyleG', 'M,L,XL', 'M', 'Purple');\n",
        "\n",
        "INSERT INTO innerwear_amazon_com VALUES\n",
        "('ProductA', '100', '80', 'url1', 'BrandA', 'Category1', 'Amazon', 'DescriptionA', 4.5, 100, 'StyleA', 'M,L', 'M', 'Red'),\n",
        "('ProductB', '200', '180', 'url2', 'BrandB', 'Category1', 'Amazon', 'DescriptionB', 4.2, 150, 'StyleB', 'S,M,L', 'S', 'Blue'),\n",
        "('ProductC', '300', '250', 'url3', 'BrandC', 'Category2', 'Amazon', 'DescriptionC', 4.8, 200, 'StyleC', 'L,XL', 'L', 'Green');\n",
        "\n",
        "INSERT INTO innerwear_macys_com VALUES\n",
        "('ProductA', '100', '85', 'url4', 'BrandA', 'Category1', 'Macys', 'DescriptionA', 4.5, 90, 'StyleA', 'M,L', 'M', 'Red'),\n",
        "('ProductD', '150', '130', 'url5', 'BrandD', 'Category3', 'Macys', 'DescriptionD', 4.0, 80, 'StyleD', 'S,M', 'S', 'Yellow'),\n",
        "('ProductE', '250', '210', 'url6', 'BrandE', 'Category4', 'Macys', 'DescriptionE', 3.9, 60, 'StyleE', 'M,L', 'L', 'Black');\n",
        "```\n",
        "\n",
        "---\n",
        "```sql\n",
        "WITH CTE AS (SELECT A.product_name, A.brand_name, A.price, A.rating, A.mrp\n",
        "FROM innerwear_amazon_com A\n",
        "LEFT JOIN innerwear_topshop_com T ON A.product_name = T.product_name AND A.mrp = T.mrp\n",
        "WHERE T.product_name IS NULL)\n",
        "SELECT A.product_name, A.brand_name, A.price, A.rating\n",
        "FROM CTE A\n",
        "LEFT JOIN innerwear_macys_com M ON A.product_name = M.product_name AND A.mrp = M.mrp\n",
        "WHERE M.product_name IS NULL;\n"
      ],
      "metadata": {
        "id": "39HK8CPWUUyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# âœ… Q5. Oracle (Hard Level)\n",
        "\n",
        "### **Problem Statement**\n",
        "\n",
        "Write a query that compares each employee's salary to their **manager's salary** and the **average department salary** (excluding the managerâ€™s salary).\n",
        "\n",
        "Display:\n",
        "\n",
        "* department\n",
        "* employee ID\n",
        "* employeeâ€™s salary\n",
        "* managerâ€™s salary\n",
        "* department average salary\n",
        "\n",
        "Order the results by **department** and then by **employee salary (highest â†’ lowest)**.\n",
        "\n",
        "ðŸ” By solving this, youâ€™ll learn **CTEs, multiple joins, and GROUP BY**.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Google Colab Implementation\n",
        "```sql\n",
        "CREATE TABLE employee_o (\n",
        "    id INT PRIMARY KEY,\n",
        "    first_name VARCHAR(50),\n",
        "    last_name VARCHAR(50),\n",
        "    age INT,\n",
        "    gender VARCHAR(10),\n",
        "    employee_title VARCHAR(50),\n",
        "    department VARCHAR(50),\n",
        "    salary INT,\n",
        "    manager_id INT\n",
        ");\n",
        "\n",
        "INSERT INTO employee_o VALUES\n",
        "(1, 'Alice', 'Smith', 45, 'F', 'Manager', 'HR', 9000, 1),\n",
        "(2, 'Bob', 'Johnson', 34, 'M', 'Assistant', 'HR', 4500, 1),\n",
        "(3, 'Charlie', 'Williams', 28, 'M', 'Coordinator', 'HR', 4800, 1),\n",
        "(4, 'Diana', 'Brown', 32, 'F', 'Manager', 'IT', 12000, 4),\n",
        "(5, 'Eve', 'Jones', 27, 'F', 'Analyst', 'IT', 7000, 4),\n",
        "(6, 'Frank', 'Garcia', 29, 'M', 'Developer', 'IT', 7500, 4),\n",
        "(7, 'Grace', 'Miller', 30, 'F', 'Manager', 'Finance', 10000, 7),\n",
        "(8, 'Hank', 'Davis', 26, 'M', 'Analyst', 'Finance', 6200, 7),\n",
        "(9, 'Ivy', 'Martinez', 31, 'F', 'Clerk', 'Finance', 5900, 7),\n",
        "(10, 'John', 'Lopez', 36, 'M', 'Manager', 'Marketing', 11000, 10),\n",
        "(11, 'Kim', 'Gonzales', 29, 'F', 'Specialist', 'Marketing', 6800, 10),\n",
        "(12, 'Leo', 'Wilson', 27, 'M', 'Coordinator', 'Marketing', 6600, 10);\n",
        "\n",
        "\n",
        "# Step 5: Query using CTEs\n",
        "\n",
        "```sql\n",
        "WITH DepartmentAvgSalary AS (\n",
        "    SELECT department,\n",
        "           AVG(salary) AS dept_avg_salary\n",
        "    FROM employee_o e\n",
        "    WHERE id != manager_id   -- exclude manager's own salary\n",
        "    GROUP BY department\n",
        "),\n",
        "ManagerSalary AS (\n",
        "    SELECT e.id AS emp_id,\n",
        "           e.department,\n",
        "           e.salary AS emp_salary,\n",
        "           m.salary AS manager_salary\n",
        "    FROM employee_o e\n",
        "    JOIN employee_o m\n",
        "      ON e.manager_id = m.id\n",
        "    WHERE e.id != e.manager_id   -- exclude managers who report to themselves\n",
        ")\n",
        "SELECT m.department,\n",
        "       m.emp_id,\n",
        "       m.emp_salary,\n",
        "       m.manager_salary,\n",
        "       d.dept_avg_salary\n",
        "FROM ManagerSalary m\n",
        "JOIN DepartmentAvgSalary d\n",
        "  ON m.department = d.department\n",
        "ORDER BY m.department, m.emp_salary DESC;\n",
        "```\n",
        "---\n",
        "\n",
        "\n",
        "## ðŸ”¹ Explanation of Query Steps\n",
        "\n",
        "1. **`DepartmentAvgSalary` CTE**\n",
        "\n",
        "   * Computes the average salary for each department **excluding the managerâ€™s own salary** (`id != manager_id`).\n",
        "\n",
        "2. **`ManagerSalary` CTE**\n",
        "\n",
        "   * Self-join on `manager_id` to fetch each employeeâ€™s managerâ€™s salary.\n",
        "   * Excludes managers who report to themselves.\n",
        "\n",
        "3. **Final Select**\n",
        "\n",
        "   * Joins both CTEs to bring **employeeâ€™s salary, managerâ€™s salary, and department average salary** together.\n",
        "   * Ordered by department, then salary (highest â†’ lowest).\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "jcaLTTsQUtp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "x8elvRmtU4t9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q6. Walmart, Paypal (Medium Level)\n",
        "\n",
        "Find managers with at least 7 direct reporting employees. In situations where user is reporting to himself/herself, count that also.\n",
        "Output first names of managers.\n",
        "\n",
        "ðŸ”By solving this, you'll learn how to use self join. Give it a try and share the output! ðŸ‘‡\n",
        "\n",
        "ð’ðœð¡ðžð¦ðš ðšð§ð ðƒðšð­ðšð¬ðžð­:\n",
        "CREATE TABLE employees (id INT PRIMARY KEY,first_name VARCHAR(50),last_name VARCHAR(50),age INT,sex VARCHAR(10),employee_title VARCHAR(50),department VARCHAR(50),salary INT,target INT,bonus INT,email VARCHAR(100),city VARCHAR(50),address VARCHAR(255),manager_id INT);\n",
        "\n",
        "INSERT INTO employees (id, first_name, last_name, age, sex, employee_title, department, salary, target, bonus, email, city, address, manager_id) VALUES(1, 'Alice', 'Smith', 40, 'F', 'Manager', 'Sales', 90000, 100000, 15000, 'alice.smith@example.com', 'New York', '123 Main St', 1),(2, 'Bob', 'Johnson', 35, 'M', 'Team Lead', 'Sales', 80000, 95000, 12000, 'bob.johnson@example.com', 'Chicago', '456 Oak St', 1),(3, 'Carol', 'Williams', 30, 'F', 'Sales Executive', 'Sales', 70000, 85000, 10000, 'carol.williams@example.com', 'New York', '789 Pine St', 1),(4, 'David', 'Brown', 28, 'M', 'Sales Executive', 'Sales', 68000, 80000, 9000, 'david.brown@example.com', 'Chicago', '101 Maple St', 1),(5, 'Emma', 'Jones', 32, 'F', 'Sales Executive', 'Sales', 71000, 86000, 9500, 'emma.jones@example.com', 'New York', '202 Cedar St', 1),(6, 'Frank', 'Miller', 45, 'M', 'Manager', 'Engineering', 95000, 105000, 16000, 'frank.miller@example.com', 'San Francisco', '303 Spruce St', 6),(7, 'Grace', 'Davis', 29, 'F', 'Engineer', 'Engineering', 73000, 87000, 11000, 'grace.davis@example.com', 'San Francisco', '404 Willow St', 6);\n",
        "-----------\n",
        "\n",
        "I have provided an explanation and query, but I encourage you to try solving it first. Later, you can check the query for reference.\n",
        "\n",
        "ð„ð±ð©ð¥ðšð§ðšð­ð¢ð¨ð§ ð­ð¨ ð’ð¨ð¥ð¯ðž ðð®ðžð«ð²\n",
        "1. Self-reporting Employees: Since manager_id points back to the id column in the same table, cases where manager_id = id (self-reporting) are automatically included.\n",
        "\n",
        "2. JOIN: The query joins the employees table with itself to match employees (e) with their managers (m) based on manager_id.\n",
        "\n",
        "3. COUNT and HAVING: After grouping by the managerâ€™s id, HAVING COUNT(e.id) >= 7 filters for managers with at least 7 direct reports."
      ],
      "metadata": {
        "id": "bReTgAc_U-RR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q7. Microsoft (Hard Level)\n",
        "\n",
        "Find the total number of downloads for paying and non-paying users by date. Include only records where non-paying customers have more downloads than paying customers. The output should be sorted by earliest date first and contain 3 columns date, non-paying downloads, paying downloads.\n",
        "\n",
        "Note: In Oracle you should use \"date\" when referring to date column (reserved keyword).\n",
        "\n",
        "ðŸ”By solving this, you'll learn how to use join, groupby and having. Give it a try and share the output!ðŸ‘‡\n",
        "\n",
        "ð’ðœð¡ðžð¦ðš ðšð§ð ðƒðšð­ðšð¬ðžð­:\n",
        "CREATE TABLE ms_user_dimension (user_id INT PRIMARY KEY,acc_id INT);\n",
        "INSERT INTO ms_user_dimension (user_id, acc_id) VALUES (1, 101),(2, 102),(3, 103),(4, 104),(5, 105);\n",
        "\n",
        "CREATE TABLE ms_acc_dimension (acc_id INT PRIMARY KEY,paying_customer VARCHAR(10));\n",
        "INSERT INTO ms_acc_dimension (acc_id, paying_customer) VALUES (101, 'Yes'),(102, 'No'),(103, 'Yes'),(104, 'No'),(105, 'No');\n",
        "\n",
        "CREATE TABLE ms_download_facts (date DATETIME,user_id INT,downloads INT);\n",
        "INSERT INTO ms_download_facts (date, user_id, downloads) VALUES ('2024-10-01', 1, 10),('2024-10-01', 2, 15),('2024-10-02', 1, 8),('2024-10-02', 3, 12),('2024-10-02', 4, 20),('2024-10-03', 2, 25),('2024-10-03', 5, 18);\n",
        "-----------\n",
        "\n",
        "I have provided an explanation and query, but I encourage you to try solving it first. Later, you can check the query for reference.\n",
        "\n",
        "ð„ð±ð©ð¥ðšð§ðšð­ð¢ð¨ð§ ð­ð¨ ð’ð¨ð¥ð¯ðž ðð®ðžð«ð²\n",
        "1. CONVERT(DATE, d.date): We convert the datetime to date to ensure weâ€™re aggregating by date only.\n",
        "\n",
        "2. SUM with CASE: The CASE within SUM helps in counting downloads conditionally:\n",
        "For non-paying customers (a.paying_customer = 'No'), it sums downloads only when the customer is non-paying.\n",
        "For paying customers (a.paying_customer = 'Yes'), it sums downloads only when the customer is paying.\n",
        "\n",
        "3. HAVING Clause: Filters results to include only dates where non-paying customers have more downloads than paying customers.\n",
        "\n",
        "4. ORDER BY [date] ASC: Orders the output by the date in ascending order, as required."
      ],
      "metadata": {
        "id": "oGHivoctVJCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##Q8. Apple | Microsoft | Dell (Easy Level)\n",
        "\n",
        "---\n",
        "\n",
        "**Write a query that returns the number of unique users per client per month.**\n",
        "---\n",
        "\n",
        "### Sample Data: `fact_events`\n",
        "\n",
        "| id | time\\_id   | user\\_id   | customer\\_id | client\\_id | event\\_type         | event\\_id |\n",
        "| -- | ---------- | ---------- | ------------ | ---------- | ------------------- | --------- |\n",
        "| 1  | 2020-02-28 | 3668-QPYBK | Sendit       | desktop    | message sent        | 3         |\n",
        "| 2  | 2020-02-28 | 7892-POOKP | Connectix    | mobile     | file received       | 2         |\n",
        "| 3  | 2020-04-03 | 9763-GRSKD | Zoomit       | desktop    | video call received | 7         |\n",
        "| 4  | 2020-04-02 | 9763-GRSKD | Connectix    | desktop    | video call received | 7         |\n",
        "| 5  | 2020-02-06 | 9237-HQITU | Sendit       | desktop    | video call received | 7         |\n",
        "| 6  | 2020-02-27 | 8191-XWSZG | Connectix    | desktop    | file received       | 2         |\n",
        "| 7  | 2020-04-03 | 9237-HQITU | Connectix    | desktop    | video call received | 7         |\n",
        "| 8  | 2020-03-01 | 9237-HQITU | Connectix    | mobile     | message received    | 4         |\n",
        "| 9  | 2020-04-02 | 4190-MFLUW | Connectix    | mobile     | video call received | 7         |\n",
        "| 10 | 2020-04-21 | 9763-GRSKD | Sendit       | desktop    | file received       | 2         |\n",
        "\n",
        "---\n",
        "\n",
        "### How to Think About the Solution\n",
        "\n",
        "1. **Extract month & year**\n",
        "   Use `FORMAT(time_id, 'yyyy-MM')` to get the month and year for grouping.\n",
        "\n",
        "2. **Count distinct users**\n",
        "   Use `COUNT(DISTINCT user_id)` to find unique users per group.\n",
        "\n",
        "3. **Group by client and month**\n",
        "   Group by both `client_id` and the formatted date.\n",
        "\n",
        "4. **Order for clarity**\n",
        "   Sort the output by `client_id` and month to make it readable.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s-2eDaQpVdQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Q9. Walmart (Hard Level)\n",
        "\n",
        "---\n",
        "\n",
        "**Identify users who started a session and placed an order on the same day. For these users, calculate the total number of orders and the total order value for that day. Your output should include the user, the session date, the total number of orders, and the total order value for that day.**\n",
        "\n",
        "---\n",
        "\n",
        "### Sample Data: `sessions`, `order_summary`\n",
        "\n",
        "#### `sessions`\n",
        "\n",
        "| session\\_id | user\\_id | session\\_date       |\n",
        "| ----------- | -------- | ------------------- |\n",
        "| 1           | 1        | 2024-01-01 00:00:00 |\n",
        "| 2           | 2        | 2024-01-02 00:00:00 |\n",
        "| 3           | 3        | 2024-01-05 00:00:00 |\n",
        "| 4           | 3        | 2024-01-05 00:00:00 |\n",
        "| 5           | 4        | 2024-01-03 00:00:00 |\n",
        "| 6           | 4        | 2024-01-03 00:00:00 |\n",
        "| 7           | 5        | 2024-01-04 00:00:00 |\n",
        "| 8           | 5        | 2024-01-04 00:00:00 |\n",
        "| 9           | 3        | 2024-01-05 00:00:00 |\n",
        "| 10          | 5        | 2024-01-04 00:00:00 |\n",
        "\n",
        "#### `order_summary`\n",
        "\n",
        "| order\\_id | user\\_id | order\\_value | order\\_date         |\n",
        "| --------- | -------- | ------------ | ------------------- |\n",
        "| 1         | 1        | 152          | 2024-01-01 00:00:00 |\n",
        "| 2         | 2        | 485          | 2024-01-02 00:00:00 |\n",
        "| 3         | 3        | 398          | 2024-01-05 00:00:00 |\n",
        "| 4         | 3        | 320          | 2024-01-05 00:00:00 |\n",
        "| 5         | 4        | 156          | 2024-01-03 00:00:00 |\n",
        "| 6         | 4        | 121          | 2024-01-03 00:00:00 |\n",
        "| 7         | 5        | 238          | 2024-01-04 00:00:00 |\n",
        "| 8         | 5        | 70           | 2024-01-04 00:00:00 |\n",
        "| 9         | 3        | 152          | 2024-01-05 00:00:00 |\n",
        "| 10        | 5        | 171          | 2024-01-04 00:00:00 |\n",
        "\n",
        "---\n",
        "\n",
        "### How to Think About the Solution\n",
        "\n",
        "1. **Join tables**\n",
        "   Join `sessions` and `order_summary` on `user_id`.\n",
        "\n",
        "2. **Match session and order dates**\n",
        "   Use `CONVERT(DATE, session_date)` and `CONVERT(DATE, order_date)` to ignore time and match by calendar day.\n",
        "\n",
        "3. **Aggregate**\n",
        "   Use `COUNT(order_id)` for number of orders, and `SUM(order_value)` for total value.\n",
        "\n",
        "4. **Group by**\n",
        "   Group by `user_id` and session date.\n",
        "\n",
        "5. **Having**\n",
        "   Filter to only include users who placed at least 1 order that day using `HAVING COUNT(order_id) > 0`.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "D2Wqand7i1-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Q10. Amazon | DoorDash (Medium Level)\n",
        "\n",
        "---\n",
        "\n",
        "**Write a query to find the job title(s) of the highest-paid employee(s). Return all titles that correspond to the highest salary in the company.**\n",
        "\n",
        "---\n",
        "\n",
        "### Sample Data: `worker`, `title`\n",
        "\n",
        "#### `worker`\n",
        "\n",
        "| worker\\_id | first\\_name | last\\_name | salary | joining\\_date | department  |\n",
        "| ---------- | ----------- | ---------- | ------ | ------------- | ----------- |\n",
        "| 1          | John        | Doe        | 80000  | 2020-01-15    | Engineering |\n",
        "| 2          | Jane        | Smith      | 120000 | 2019-03-10    | Marketing   |\n",
        "| 3          | Alice       | Brown      | 120000 | 2021-06-21    | Sales       |\n",
        "| 4          | Bob         | Davis      | 75000  | 2018-04-30    | Engineering |\n",
        "| 5          | Charlie     | Miller     | 95000  | 2021-01-15    | Sales       |\n",
        "\n",
        "#### `title`\n",
        "\n",
        "| worker\\_ref\\_id | worker\\_title      | affected\\_from |\n",
        "| --------------- | ------------------ | -------------- |\n",
        "| 1               | Engineer           | 2020-01-15     |\n",
        "| 2               | Marketing Manager  | 2019-03-10     |\n",
        "| 3               | Sales Manager      | 2021-06-21     |\n",
        "| 4               | Junior Engineer    | 2018-04-30     |\n",
        "| 5               | Senior Salesperson | 2021-01-15     |\n",
        "\n",
        "---\n",
        "\n",
        "### How to Think About the Solution\n",
        "\n",
        "1. **Find the max salary**\n",
        "   Use a subquery like `(SELECT MAX(salary) FROM worker)` to get the highest salary.\n",
        "\n",
        "2. **Join tables**\n",
        "   Join `worker` and `title` using `worker.worker_id = title.worker_ref_id` to associate each worker with their title.\n",
        "\n",
        "3. **Filter**\n",
        "   Use `WHERE salary = (SELECT MAX(salary) ...)` to select only the highest-paid worker(s).\n",
        "\n",
        "4. **Select title**\n",
        "   Return the `worker_title` of the filtered records.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "0QPyl8Zbjhzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q11. Uber | Medium Level\n",
        "\n",
        "---\n",
        "\n",
        "**Write a query to find the top 3 business purpose categories that generate the most miles driven. Only include rides where the category is 'Business'.**\n",
        "\n",
        "---\n",
        "\n",
        "### Sample Data: `my_uber_drives`\n",
        "\n",
        "| start\\_date      | end\\_date        | category | start           | stop            | miles | purpose         |\n",
        "| ---------------- | ---------------- | -------- | --------------- | --------------- | ----- | --------------- |\n",
        "| 2016-01-01 21:11 | 2016-01-01 21:17 | Business | Fort Pierce     | Fort Pierce     | 5.1   | Meal/Entertain  |\n",
        "| 2016-01-02 01:25 | 2016-01-02 01:37 | Business | Fort Pierce     | Fort Pierce     | 5     | NULL            |\n",
        "| 2016-01-02 20:25 | 2016-01-02 20:38 | Business | Fort Pierce     | Fort Pierce     | 4.8   | Errand/Supplies |\n",
        "| 2016-01-05 17:31 | 2016-01-05 17:45 | Business | Fort Pierce     | Fort Pierce     | 4.7   | Meeting         |\n",
        "| 2016-01-06 14:42 | 2016-01-06 15:49 | Business | Fort Pierce     | West Palm Beach | 63.7  | Customer Visit  |\n",
        "| 2016-01-06 17:15 | 2016-01-06 17:19 | Business | West Palm Beach | West Palm Beach | 4.3   | Meal/Entertain  |\n",
        "| 2016-01-06 17:30 | 2016-01-06 17:35 | Business | West Palm Beach | Palm Beach      | 7.1   | Meeting         |\n",
        "\n",
        "---\n",
        "\n",
        "### How to Think About the Solution\n",
        "\n",
        "1. **Filter on category**\n",
        "   Use `WHERE category = 'Business'` to focus only on business trips.\n",
        "\n",
        "2. **Group by purpose**\n",
        "   Aggregate total miles using `SUM(miles)` grouped by each `purpose`.\n",
        "\n",
        "3. **Order results**\n",
        "   Use `ORDER BY total_miles DESC` to rank by mileage.\n",
        "\n",
        "4. **Limit to Top 3**\n",
        "   Use `LIMIT 3` to return the top 3 business purpose categories.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "W0Bg6D32kDaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k7u5lVyLj05N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Q12. JP Morgan | Chase | Bloomberg (Medium Level)\n",
        "\n",
        "---\n",
        "\n",
        "**Write a query to find all invalid transactions in December 2022. A transaction is considered invalid if it occurs outside business hours (Monâ€“Fri, 09:00â€“16:00), on weekends, or on Irish public holidays (25th and 26th December). Return the `transaction_id` of such records.**\n",
        "\n",
        "---\n",
        "\n",
        "### Sample Data: `boi_transactions`\n",
        "\n",
        "| transaction\\_id | time\\_stamp      |\n",
        "| --------------- | ---------------- |\n",
        "| 1051            | 2022-12-03 10:15 |\n",
        "| 1052            | 2022-12-03 17:00 |\n",
        "| 1053            | 2022-12-04 10:00 |\n",
        "| 1054            | 2022-12-04 14:00 |\n",
        "| 1055            | 2022-12-05 08:59 |\n",
        "| 1056            | 2022-12-05 16:01 |\n",
        "| 1057            | 2022-12-06 09:00 |\n",
        "| 1058            | 2022-12-06 15:59 |\n",
        "| 1059            | 2022-12-07 12:00 |\n",
        "| 1060            | 2022-12-08 09:00 |\n",
        "| 1061            | 2022-12-09 10:00 |\n",
        "| 1062            | 2022-12-10 11:00 |\n",
        "| 1063            | 2022-12-10 17:30 |\n",
        "| 1064            | 2022-12-11 12:00 |\n",
        "| 1065            | 2022-12-12 08:59 |\n",
        "| 1066            | 2022-12-12 16:01 |\n",
        "| 1067            | 2022-12-25 10:00 |\n",
        "| 1068            | 2022-12-25 15:00 |\n",
        "| 1069            | 2022-12-26 09:00 |\n",
        "| 1070            | 2022-12-26 14:00 |\n",
        "| 1071            | 2022-12-26 16:30 |\n",
        "| 1072            | 2022-12-27 09:00 |\n",
        "| 1073            | 2022-12-28 08:30 |\n",
        "| 1074            | 2022-12-29 16:15 |\n",
        "| 1075            | 2022-12-30 14:00 |\n",
        "| 1076            | 2022-12-31 10:00 |\n",
        "\n",
        "---\n",
        "\n",
        "### How to Think About the Solution\n",
        "\n",
        "1. **Filter December 2022**\n",
        "   Use `MONTH(time_stamp) = 12 AND YEAR(time_stamp) = 2022` to isolate December transactions.\n",
        "\n",
        "2. **Check for weekends**\n",
        "   Use `DATEPART(WEEKDAY, time_stamp)` to identify Sundays and Saturdays.\n",
        "\n",
        "3. **Check for time range**\n",
        "   Use `CAST(time_stamp AS TIME) < '09:00:00' OR > '16:00:00'` to flag anything outside business hours.\n",
        "\n",
        "4. **Check for holidays**\n",
        "   Add an extra condition for `DAY(time_stamp) IN (25, 26)` to filter Irish public holidays.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "imC_nRsVkudS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Q13. Google (Medium Level)\n",
        "\n",
        "---\n",
        "\n",
        "**Find all records from days when the number of distinct users receiving emails was greater than the number of distinct users sending emails.**\n",
        "\n",
        "---\n",
        "\n",
        "### Sample Data: `google_gmail_emails`\n",
        "\n",
        "| id | from\\_user         | to\\_user           | day |\n",
        "| -- | ------------------ | ------------------ | --- |\n",
        "| 0  | 6edf0be4b2267df1fa | 75d295377a46f83236 | 10  |\n",
        "| 1  | 6edf0be4b2267df1fa | 32ded68d89443e808  | 6   |\n",
        "| 2  | 6edf0be4b2267df1fa | 55e60cfcc9dc49c17e | 10  |\n",
        "| 3  | 6edf0be4b2267df1fa | e0e0defbb9ec47f6f7 | 6   |\n",
        "| 4  | 6edf0be4b2267df1fa | 47be2887786891367e | 1   |\n",
        "| 5  | 6edf0be4b2267df1fa | 2813e59cf6c1ff698e | 6   |\n",
        "| 6  | 6edf0be4b2267df1fa | a84065b7933ad01019 | 8   |\n",
        "| 7  | 6edf0be4b2267df1fa | 850badf89ed8f06854 | 1   |\n",
        "| 8  | 6edf0be4b2267df1fa | 6b503743a13d778200 | 1   |\n",
        "| 9  | 6edf0be4b2267df1fa | d63386c884aeb9f71d | 3   |\n",
        "| 10 | 6edf0be4b2267df1fa | 5b8754928306a18b68 | 2   |\n",
        "| 11 | 6edf0be4b2267df1fa | 6edf0be4b2267df1fa | 8   |\n",
        "| 12 | 6edf0be4b2267df1fa | 406539987dd9b679c0 | 9   |\n",
        "| 13 | 6edf0be4b2267df1fa | 114bafadff2d882864 | 5   |\n",
        "| 14 | 6edf0be4b2267df1fa | 157e3e9278e32aba3e | 2   |\n",
        "| 15 | 75d295377a46f83236 | 75d295377a46f83236 | 6   |\n",
        "| 16 | 75d295377a46f83236 | d63386c884aeb9f71d | 8   |\n",
        "| 17 | 75d295377a46f83236 | 55e60cfcc9dc49c17e | 3   |\n",
        "| 18 | 75d295377a46f83236 | 47be2887786891367e | 10  |\n",
        "| 19 | 75d295377a46f83236 | 5b8754928306a18b68 | 10  |\n",
        "| 20 | 75d295377a46f83236 | 850badf89ed8f06854 | 7   |\n",
        "\n",
        "---\n",
        "\n",
        "### How to Think About the Solution\n",
        "\n",
        "1. **Calculate distinct counts per day**\n",
        "   Use a CTE or subqueries to find the number of distinct `to_user` (receivers) and `from_user` (senders) per `day`.\n",
        "\n",
        "2. **Compare distinct counts**\n",
        "   Select days where the number of distinct receivers is greater than distinct senders.\n",
        "\n",
        "3. **Join back to original**\n",
        "   Retrieve all records from those filtered days by joining on the `day` field.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "TkQsOZw3lbeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Q14. Amazon | Salesforce (Basic Level)\n",
        "\n",
        "---\n",
        "\n",
        "**What is the total sales revenue of Samantha and Lisa?**\n",
        "\n",
        "---\n",
        "\n",
        "### Sample Data: `sales_performance`\n",
        "\n",
        "| salesperson | widget\\_sales | sales\\_revenue | id |\n",
        "| ----------- | ------------- | -------------- | -- |\n",
        "| Jim         | 810           | 40500          | 1  |\n",
        "| Bobby       | 661           | 33050          | 2  |\n",
        "| Samantha    | 1006          | 50300          | 3  |\n",
        "| Taylor      | 984           | 49200          | 4  |\n",
        "| Tom         | 403           | 20150          | 5  |\n",
        "| Pat         | 715           | 35750          | 6  |\n",
        "| Lisa        | 1247          | 62350          | 7  |\n",
        "\n",
        "---\n",
        "\n",
        "### How to Think About the Solution\n",
        "\n",
        "1. **Filter for Samantha and Lisa**\n",
        "   Use `WHERE salesperson IN ('Samantha', 'Lisa')` to target these two salespeople.\n",
        "\n",
        "2. **Sum sales revenue**\n",
        "   Use `SUM(sales_revenue)` to get their combined sales revenue.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "_sJ-tU4Jlqf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Q15. Expedia | Airbnb (Basic Level)\n",
        "\n",
        "---\n",
        "\n",
        "**Find the number of rows for each review score earned by 'Hotel Arena'. Output the hotel name (which should be 'Hotel Arena'), review score, and the count of rows with that score for the specified hotel.**\n",
        "\n",
        "---\n",
        "\n",
        "### Sample Data: `hotel_reviews`\n",
        "\n",
        "| hotel\\_address | additional\\_number\\_of\\_scoring | review\\_date | average\\_score | hotel\\_name | reviewer\\_nationality | negative\\_review | review\\_total\\_negative\\_word\\_counts | total\\_number\\_of\\_reviews | positive\\_review | review\\_total\\_positive\\_word\\_counts | total\\_number\\_of\\_reviews\\_reviewer\\_has\\_given | reviewer\\_score | tags          | days\\_since\\_review | lat     | lng      |\n",
        "| -------------- | ------------------------------- | ------------ | -------------- | ----------- | --------------------- | ---------------- | ------------------------------------- | -------------------------- | ---------------- | ------------------------------------- | ------------------------------------------------ | --------------- | ------------- | ------------------- | ------- | -------- |\n",
        "| 123 Main St    | 5                               | 2024-01-01   | 8.5            | Hotel Arena | American              | Noisy room       | 3                                     | 200                        | Great staff      | 5                                     | 10                                               | 8.0             | Family stay   | 100 days            | 40.7128 | -74.0060 |\n",
        "| 123 Main St    | 2                               | 2024-01-02   | 8.5            | Hotel Arena | British               | Small bathroom   | 2                                     | 200                        | Clean room       | 6                                     | 5                                                | 9.0             | Solo traveler | 95 days             | 40.7128 | -74.0060 |\n",
        "| 123 Main St    | 3                               | 2024-01-03   | 8.5            | Hotel Arena | Canadian              | Slow service     | 4                                     | 200                        | Nice view        | 7                                     | 3                                                | 6.0             | Couple stay   | 90 days             | 40.7128 | -74.0060 |\n",
        "\n",
        "---\n",
        "\n",
        "### How to Think About the Solution\n",
        "\n",
        "1. **Filter by hotel name**\n",
        "   Use `WHERE hotel_name = 'Hotel Arena'` to limit rows to that hotel.\n",
        "\n",
        "2. **Group by reviewer\\_score**\n",
        "   Group the filtered rows by `reviewer_score`.\n",
        "\n",
        "3. **Count rows per score**\n",
        "   Use `COUNT(*)` to find how many reviews correspond to each score.\n",
        "\n",
        "4. **Select hotel name, score, and count**\n",
        "   Output the `hotel_name`, `reviewer_score`, and the count of rows per score.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "il_dVZvPl4lx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Q16. LinkedIn | Dropbox (Basic Level)\n",
        "\n",
        "---\n",
        "\n",
        "**Write a query that calculates the absolute difference between the highest salaries found in the marketing and engineering departments. Output just the absolute difference in salaries.**\n",
        "\n",
        "---\n",
        "\n",
        "### Sample Data: `db_employee`\n",
        "\n",
        "| id    | first\\_name | last\\_name  | salary | department\\_id |\n",
        "| ----- | ----------- | ----------- | ------ | -------------- |\n",
        "| 10306 | Ashley      | Li          | 28516  | 4              |\n",
        "| 10307 | Joseph      | Solomon     | 19945  | 1              |\n",
        "| 10311 | Melissa     | Holmes      | 33575  | 1              |\n",
        "| 10316 | Beth        | Torres      | 34902  | 1              |\n",
        "| 10317 | Pamela      | Rodriguez   | 48187  | 4              |\n",
        "| 10320 | Gregory     | Cook        | 22681  | 4              |\n",
        "| 10324 | William     | Brewer      | 15947  | 1              |\n",
        "| 10329 | Christopher | Ramos       | 37710  | 4              |\n",
        "| 10333 | Jennifer    | Blankenship | 13433  | 4              |\n",
        "| 10339 | Robert      | Mills       | 13188  | 1              |\n",
        "\n",
        "---\n",
        "\n",
        "### Sample Data: `db_dept`\n",
        "\n",
        "| id | department     |\n",
        "| -- | -------------- |\n",
        "| 1  | engineering    |\n",
        "| 2  | human resource |\n",
        "| 3  | operation      |\n",
        "| 4  | marketing      |\n",
        "\n",
        "---\n",
        "\n",
        "### How to Think About the Solution\n",
        "\n",
        "1. **Use CASE**\n",
        "   Select salaries based on department, using a CASE statement to separate marketing and engineering salaries.\n",
        "\n",
        "2. **Find MAX salaries**\n",
        "   Use `MAX()` to get the highest salary for each department.\n",
        "\n",
        "3. **Calculate absolute difference**\n",
        "   Use `ABS()` function to get the absolute difference between the two highest salaries.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ZsV5Ghrml_OR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Q17. Nvidia | Microsoft (Medium Level)\n",
        "\n",
        "---\n",
        "\n",
        "**Find the number of transactions that occurred for each product. Output the product name along with the corresponding number of transactions and order records by the product id in ascending order. You can ignore products without transactions.**\n",
        "\n",
        "---\n",
        "\n",
        "### Sample Data: `excel_sql_inventory_data`\n",
        "\n",
        "| product\\_id | product\\_name | product\\_type | unit | price\\_unit | wholesale | current\\_inventory |\n",
        "| ----------- | ------------- | ------------- | ---- | ----------- | --------- | ------------------ |\n",
        "| 1           | strawberry    | produce       | lb   | 3.28        | 1.77      | 13                 |\n",
        "| 2           | apple\\_fuji   | produce       | lb   | 1.44        | 0.43      | 2                  |\n",
        "| 3           | orange        | produce       | lb   | 1.02        | 0.37      | 2                  |\n",
        "| 4           | clementines   | produce       | lb   | 1.19        | 0.44      | 44                 |\n",
        "| 5           | blood\\_orange | produce       | lb   | 3.86        | 1.66      | 19                 |\n",
        "\n",
        "---\n",
        "\n",
        "### Sample Data: `excel_sql_transaction_data`\n",
        "\n",
        "| transaction\\_id | time                | product\\_id |\n",
        "| --------------- | ------------------- | ----------- |\n",
        "| 153             | 2016-01-06 08:57:52 | 1           |\n",
        "| 91              | 2016-01-07 12:17:27 | 1           |\n",
        "| 31              | 2016-01-05 13:19:25 | 1           |\n",
        "| 24              | 2016-01-03 10:47:44 | 3           |\n",
        "| 4               | 2016-01-06 17:57:42 | 3           |\n",
        "| 163             | 2016-01-03 10:11:22 | 3           |\n",
        "| 92              | 2016-01-08 12:03:20 | 2           |\n",
        "| 32              | 2016-01-04 19:37:14 | 4           |\n",
        "| 253             | 2016-01-06 14:15:20 | 5           |\n",
        "| 118             | 2016-01-06 14:27:33 | 5           |\n",
        "\n",
        "---\n",
        "\n",
        "### How to Think About the Solution\n",
        "\n",
        "1. **Join the tables**\n",
        "   Perform an INNER JOIN between `excel_sql_inventory_data` (alias `inv`) and `excel_sql_transaction_data` (alias `trans`) on `product_id`. This excludes products without transactions.\n",
        "\n",
        "2. **Count transactions**\n",
        "   Use `COUNT(trans.transaction_id)` to find the number of transactions per product.\n",
        "\n",
        "3. **Group and order**\n",
        "   GROUP BY `inv.product_id`, `inv.product_name` to get counts per product, and ORDER BY `inv.product_id` ASC.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "UHunERjcmb2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Q18. Amazon (Medium Level)\n",
        "\n",
        "---\n",
        "\n",
        "**Write a query that'll identify returning active users. A returning active user is a user that has made a second purchase within 7 days of any other of their purchases. Output a list of user\\_ids of these returning active users.**\n",
        "\n",
        "---\n",
        "\n",
        "### Sample Data: `amazon_transactions`\n",
        "\n",
        "| id | user\\_id | item    | created\\_at         | revenue |\n",
        "| -- | -------- | ------- | ------------------- | ------- |\n",
        "| 1  | 109      | milk    | 2020-03-03 00:00:00 | 123     |\n",
        "| 2  | 139      | biscuit | 2020-03-18 00:00:00 | 421     |\n",
        "| 3  | 120      | milk    | 2020-03-18 00:00:00 | 176     |\n",
        "| 4  | 108      | banana  | 2020-03-18 00:00:00 | 862     |\n",
        "| 5  | 130      | milk    | 2020-03-28 00:00:00 | 333     |\n",
        "| 6  | 103      | bread   | 2020-03-29 00:00:00 | 862     |\n",
        "| 7  | 122      | banana  | 2020-03-07 00:00:00 | 952     |\n",
        "| 8  | 125      | bread   | 2020-03-13 00:00:00 | 317     |\n",
        "| 9  | 139      | bread   | 2020-03-30 00:00:00 | 929     |\n",
        "| 10 | 141      | banana  | 2020-03-17 00:00:00 | 812     |\n",
        "| 11 | 116      | bread   | 2020-03-31 00:00:00 | 226     |\n",
        "| 12 | 128      | bread   | 2020-03-04 00:00:00 | 112     |\n",
        "| 13 | 146      | biscuit | 2020-03-04 00:00:00 | 362     |\n",
        "| 14 | 119      | banana  | 2020-03-28 00:00:00 | 127     |\n",
        "| 15 | 142      | bread   | 2020-03-09 00:00:00 | 503     |\n",
        "| 16 | 122      | bread   | 2020-03-06 00:00:00 | 593     |\n",
        "| 17 | 128      | biscuit | 2020-03-24 00:00:00 | 160     |\n",
        "| 18 | 112      | banana  | 2020-03-24 00:00:00 | 262     |\n",
        "| 19 | 149      | banana  | 2020-03-29 00:00:00 | 382     |\n",
        "| 20 | 100      | banana  | 2020-03-18 00:00:00 | 599     |\n",
        "\n",
        "---\n",
        "\n",
        "### How to Think About the Solution\n",
        "\n",
        "1. **Self-Join:**\n",
        "   Join the table with itself on `user_id` to compare each purchase against others by the same user.\n",
        "\n",
        "2. **Conditions:**\n",
        "\n",
        "   * `a.user_id = b.user_id` ensures the same user.\n",
        "   * `a.created_at < b.created_at` ensures the second purchase is after the first.\n",
        "   * `DATEDIFF(day, a.created_at, b.created_at) <= 7` filters for a second purchase within 7 days.\n",
        "\n",
        "3. **Output:**\n",
        "   Use `DISTINCT` to return unique user\\_ids of those who made a qualifying second purchase.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "FfsRv2y7m1CL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Q19. Netflix (Hard Level)\n",
        "\n",
        "---\n",
        "\n",
        "**Find the genre of the person with the most number of Oscar winnings.**\n",
        "If multiple people have the same highest number of wins, return the one whose name is first alphabetically. Use the names as keys when joining tables.\n",
        "\n",
        "---\n",
        "\n",
        "### Sample Data\n",
        "\n",
        "**nominee\\_information**\n",
        "\n",
        "| name              | amg\\_person\\_id | top\\_genre | birthday   | id  |\n",
        "| ----------------- | --------------- | ---------- | ---------- | --- |\n",
        "| Jennifer Lawrence | P562566         | Drama      | 1990-08-15 | 755 |\n",
        "| Jonah Hill        | P418718         | Comedy     | 1983-12-20 | 747 |\n",
        "| Anne Hathaway     | P292630         | Drama      | 1982-11-12 | 744 |\n",
        "| Jennifer Hudson   | P454405         | Drama      | 1981-09-12 | 742 |\n",
        "| Rinko Kikuchi     | P475244         | Drama      | 1981-01-06 | 739 |\n",
        "\n",
        "---\n",
        "\n",
        "**oscar\\_nominees**\n",
        "\n",
        "| year | category                     | nominee           | movie                   | winner | id   |\n",
        "| ---- | ---------------------------- | ----------------- | ----------------------- | ------ | ---- |\n",
        "| 2008 | actress in a leading role    | Anne Hathaway     | Rachel Getting Married  | 0      | 77   |\n",
        "| 2012 | actress in a supporting role | Anne HathawayLes  | Mis\\_rables             | 1      | 78   |\n",
        "| 2006 | actress in a supporting role | Jennifer Hudson   | Dreamgirls              | 1      | 711  |\n",
        "| 2010 | actress in a leading role    | Jennifer Lawrence | Winters Bone            | 1      | 717  |\n",
        "| 2012 | actress in a leading role    | Jennifer Lawrence | Silver Linings Playbook | 1      | 718  |\n",
        "| 2011 | actor in a supporting role   | Jonah Hill        | Moneyball               | 0      | 799  |\n",
        "| 2006 | actress in a supporting role | Rinko Kikuchi     | Babel                   | 0      | 1253 |\n",
        "\n",
        "---\n",
        "\n",
        "### Explanation\n",
        "\n",
        "1. **WinnerCount CTE**:\n",
        "   Calculate the total Oscar wins per nominee by counting rows with `winner = 1`.\n",
        "\n",
        "2. **Final Selection**:\n",
        "\n",
        "   * Join the winner counts with `nominee_information` on nominee name.\n",
        "   * Order by `total_wins` descending and `name` ascending.\n",
        "   * Return the top resultâ€™s `top_genre`.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "aXmBQ8aQnd0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Q20. Tesla (Medium Level)\n",
        "\n",
        "---\n",
        "\n",
        "**You are given a table of product launches by company by year. Write a query to count the net difference between the number of products companies launched in 2020 with the number of products companies launched in the previous year. Output the name of the companies and a net difference of net products released for 2020 compared to the previous year.**\n",
        "\n",
        "---\n",
        "\n",
        "### Schema and Dataset\n",
        "\n",
        "**car\\_launches**\n",
        "\n",
        "| year | company\\_name | product\\_name |\n",
        "| ---- | ------------- | ------------- |\n",
        "| 2019 | Toyota        | Avalon        |\n",
        "| 2019 | Toyota        | Camry         |\n",
        "| 2020 | Toyota        | Corolla       |\n",
        "| 2019 | Honda         | Accord        |\n",
        "| 2019 | Honda         | Passport      |\n",
        "| 2019 | Honda         | CR-V          |\n",
        "| 2020 | Honda         | Pilot         |\n",
        "| 2019 | Honda         | Civic         |\n",
        "| 2020 | Chevrolet     | Trailblazer   |\n",
        "| 2020 | Chevrolet     | Trax          |\n",
        "| 2019 | Chevrolet     | Traverse      |\n",
        "| 2020 | Chevrolet     | Blazer        |\n",
        "| 2019 | Ford          | Figo          |\n",
        "| 2020 | Ford          | Aspire        |\n",
        "| 2019 | Ford          | Endeavour     |\n",
        "| 2020 | Jeep          | Wrangler      |\n",
        "\n",
        "---\n",
        "\n",
        "### Explanation to Solve Query\n",
        "\n",
        "1. **Counting Products per Year:**\n",
        "   Use `SUM` with `CASE` statements to count how many products each company launched in 2020 and in 2019.\n",
        "\n",
        "2. **Calculating Net Difference:**\n",
        "   Compute the difference between product counts in 2020 and 2019 for each company to get the net change.\n",
        "\n",
        "3. **Ordering:**\n",
        "   Order the results by `net_difference` descending so companies with the biggest increase appear first.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "BkDUZzxPnht9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Q21. IBM (Hard Level)\n",
        "\n",
        "---\n",
        "\n",
        "**IBM is working on a new feature to analyze user purchasing behavior for all Fridays in the first quarter of the year. For each Friday separately, calculate the average amount users have spent per order. The output should contain the week number of that Friday and average amount spent.**\n",
        "\n",
        "---\n",
        "\n",
        "### Schema and Dataset\n",
        "\n",
        "**user\\_purchases**\n",
        "\n",
        "| user\\_id | date       | amount\\_spent | day\\_name |\n",
        "| -------- | ---------- | ------------- | --------- |\n",
        "| 1047     | 2023-01-01 | 288           | Sunday    |\n",
        "| 1099     | 2023-01-04 | 803           | Wednesday |\n",
        "| 1055     | 2023-01-07 | 546           | Saturday  |\n",
        "| 1040     | 2023-01-10 | 680           | Tuesday   |\n",
        "| 1052     | 2023-01-13 | 889           | Friday    |\n",
        "| 1052     | 2023-01-13 | 596           | Friday    |\n",
        "| 1016     | 2023-01-16 | 960           | Monday    |\n",
        "| 1023     | 2023-01-17 | 861           | Tuesday   |\n",
        "| 1010     | 2023-01-19 | 758           | Thursday  |\n",
        "| 1013     | 2023-01-19 | 346           | Thursday  |\n",
        "| 1069     | 2023-01-21 | 541           | Saturday  |\n",
        "| 1030     | 2023-01-22 | 175           | Sunday    |\n",
        "| 1034     | 2023-01-23 | 707           | Monday    |\n",
        "| 1019     | 2023-01-25 | 253           | Wednesday |\n",
        "| 1052     | 2023-01-25 | 868           | Wednesday |\n",
        "| 1095     | 2023-01-27 | 424           | Friday    |\n",
        "| 1017     | 2023-01-28 | 755           | Saturday  |\n",
        "| 1010     | 2023-01-29 | 615           | Sunday    |\n",
        "| 1063     | 2023-01-31 | 534           | Tuesday   |\n",
        "| 1019     | 2023-02-03 | 185           | Friday    |\n",
        "| 1019     | 2023-02-03 | 995           | Friday    |\n",
        "| 1092     | 2023-02-06 | 796           | Monday    |\n",
        "| 1058     | 2023-02-09 | 384           | Thursday  |\n",
        "| 1055     | 2023-02-12 | 319           | Sunday    |\n",
        "| 1090     | 2023-02-15 | 168           | Wednesday |\n",
        "| 1090     | 2023-02-18 | 146           | Saturday  |\n",
        "| 1062     | 2023-02-21 | 193           | Tuesday   |\n",
        "| 1023     | 2023-02-24 | 259           | Friday    |\n",
        "\n",
        "---\n",
        "\n",
        "### Explanation to Solve Query\n",
        "\n",
        "1. Identify the Fridays in the first quarter (Q1) of the year.\n",
        "\n",
        "2. Calculate the week number for each Friday.\n",
        "\n",
        "3. Group purchases by week number and calculate the average amount spent per order.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "5c__vx2dpUBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Q22. Microsoft (Medium Level)\n",
        "\n",
        "---\n",
        "\n",
        "**Given a list of projects and employees mapped to each project, calculate by the amount of project budget allocated to each employee. The output should include the project title and the project budget rounded to the closest integer. Order your list by projects with the highest budget per employee first.**\n",
        "\n",
        "---\n",
        "\n",
        "### Schema and Dataset\n",
        "\n",
        "**ms\\_projects**\n",
        "\n",
        "| id | title     | budget |\n",
        "| -- | --------- | ------ |\n",
        "| 1  | Project1  | 29498  |\n",
        "| 2  | Project2  | 32487  |\n",
        "| 3  | Project3  | 43909  |\n",
        "| 4  | Project4  | 15776  |\n",
        "| 5  | Project5  | 36268  |\n",
        "| 6  | Project6  | 41611  |\n",
        "| 7  | Project7  | 34003  |\n",
        "| 8  | Project8  | 49284  |\n",
        "| 9  | Project9  | 32341  |\n",
        "| 10 | Project10 | 47587  |\n",
        "| 11 | Project11 | 11705  |\n",
        "| 12 | Project12 | 10468  |\n",
        "| 13 | Project13 | 43238  |\n",
        "| 14 | Project14 | 30014  |\n",
        "| 15 | Project15 | 48116  |\n",
        "| 16 | Project16 | 19922  |\n",
        "| 17 | Project17 | 19061  |\n",
        "| 18 | Project18 | 10302  |\n",
        "| 19 | Project19 | 44986  |\n",
        "| 20 | Project20 | 19497  |\n",
        "\n",
        "**ms\\_emp\\_projects**\n",
        "\n",
        "| emp\\_id | project\\_id |\n",
        "| ------- | ----------- |\n",
        "| 10592   | 1           |\n",
        "| 10593   | 2           |\n",
        "| 10594   | 3           |\n",
        "| 10595   | 4           |\n",
        "| 10596   | 5           |\n",
        "| 10597   | 6           |\n",
        "| 10598   | 7           |\n",
        "| 10599   | 8           |\n",
        "| 10600   | 9           |\n",
        "| 10601   | 10          |\n",
        "| 10602   | 11          |\n",
        "| 10603   | 12          |\n",
        "| 10604   | 13          |\n",
        "| 10605   | 14          |\n",
        "| 10606   | 15          |\n",
        "| 10607   | 16          |\n",
        "| 10608   | 17          |\n",
        "| 10609   | 18          |\n",
        "| 10610   | 19          |\n",
        "| 10611   | 20          |\n",
        "\n",
        "---\n",
        "\n",
        "### Explanation of the Query\n",
        "\n",
        "1. **Joining Tables:** The initial step involves joining the `ms_projects` and `ms_emp_projects` tables on the project ID to combine project details (including titles and budgets) with employee assignments.\n",
        "\n",
        "2. **Grouping and Aggregating:** The data is then grouped by project title and budget, allowing for the calculation of budget per employee by dividing the total budget of each project by the count of employees assigned to that project.\n",
        "\n",
        "3. **Rounding and Ordering:** Finally, the computed budget per employee is rounded to the nearest integer, and the results are ordered in descending order to prioritize projects with the highest budget allocation per employee.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "nD7FoKK3p9Kt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Q23. Airbnb (Medium Level)\n",
        "\n",
        "---\n",
        "\n",
        "**Find the total number of available beds per hosts' nationality.\n",
        "Output the nationality along with the corresponding total number of available beds.\n",
        "Sort records by the total available beds in descending order.**\n",
        "\n",
        "---\n",
        "\n",
        "### Schema and Dataset\n",
        "\n",
        "**airbnb\\_apartments**\n",
        "\n",
        "| host\\_id | apartment\\_id | apartment\\_type | n\\_beds | n\\_bedrooms | country | city      |\n",
        "| -------- | ------------- | --------------- | ------- | ----------- | ------- | --------- |\n",
        "| 0        | A1            | Room            | 1       | 1           | USA     | NewYork   |\n",
        "| 0        | A2            | Room            | 1       | 1           | USA     | NewJersey |\n",
        "| 0        | A3            | Room            | 1       | 1           | USA     | NewJersey |\n",
        "| 1        | A4            | Apartment       | 2       | 1           | USA     | Houston   |\n",
        "| 1        | A5            | Apartment       | 2       | 1           | USA     | LasVegas  |\n",
        "| 3        | A7            | Penthouse       | 3       | 3           | China   | Tianjin   |\n",
        "| 3        | A8            | Penthouse       | 5       | 5           | China   | Beijing   |\n",
        "| 4        | A9            | Apartment       | 2       | 1           | Mali    | Bamako    |\n",
        "| 5        | A10           | Room            | 3       | 1           | Mali    | Segou     |\n",
        "\n",
        "**airbnb\\_hosts**\n",
        "\n",
        "| host\\_id | nationality | gender | age |\n",
        "| -------- | ----------- | ------ | --- |\n",
        "| 0        | USA         | M      | 28  |\n",
        "| 1        | USA         | F      | 29  |\n",
        "| 2        | China       | F      | 31  |\n",
        "| 3        | China       | M      | 24  |\n",
        "| 4        | Mali        | M      | 30  |\n",
        "| 5        | Mali        | F      | 30  |\n",
        "\n",
        "---\n",
        "\n",
        "### Explanation of the Query\n",
        "\n",
        "1. **Joining Tables:** The first step involves joining the `airbnb_apartments` and `airbnb_hosts` tables on the `host_id`. This allows us to combine apartment details (like `n_beds`) with the host's nationality.\n",
        "\n",
        "2. **Grouping and Aggregating:** We group the combined data by nationality to calculate the total number of available beds (`SUM(n_beds)`) per nationality.\n",
        "\n",
        "3. **Sorting the Results:** The final results are sorted in descending order by the total number of beds so that nationalities with the most available beds appear at the top.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9zm1YY4iqJon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Here's your formatted entry with a sequence number and structured just like your previous ones:\n",
        "\n",
        "---\n",
        "\n",
        "## Q24. Uber (Hard Level)\n",
        "\n",
        "---\n",
        "\n",
        "**Some forecasting methods are extremely simple and surprisingly effective. NaÃ¯ve forecast is one of them. To create a naÃ¯ve forecast for \"distance per dollar\" (defined as `distance_to_travel / monetary_cost`), first sum the \"distance to travel\" and \"monetary cost\" values monthly. This gives the actual value for the current month. For the forecasted value, use the previous month's value.**\n",
        "\n",
        "**After obtaining both actual and forecasted values, calculate the root mean squared error (RMSE) using the formula:**\n",
        "\n",
        "> `RMSE = sqrt(mean(square(actual - forecast)))`\n",
        "\n",
        "**Report the RMSE rounded to two decimal places.**\n",
        "\n",
        "---\n",
        "\n",
        "### Schema and Dataset\n",
        "\n",
        "**uber\\_request\\_logs**\n",
        "\n",
        "| request\\_id | request\\_date       | request\\_status | distance\\_to\\_travel | monetary\\_cost | driver\\_to\\_client\\_distance |\n",
        "| ----------- | ------------------- | --------------- | -------------------- | -------------- | ---------------------------- |\n",
        "| 1           | 2020-01-09 00:00:00 | success         | 70.59                | 6.56           | 14.36                        |\n",
        "| 2           | 2020-01-24 00:00:00 | success         | 93.36                | 22.68          | 19.9                         |\n",
        "| 3           | 2020-02-08 00:00:00 | fail            | 51.24                | 11.39          | 21.32                        |\n",
        "| 4           | 2020-02-23 00:00:00 | success         | 61.58                | 8.04           | 44.26                        |\n",
        "| 5           | 2020-03-09 00:00:00 | success         | 25.04                | 7.19           | 1.74                         |\n",
        "| 6           | 2020-03-24 00:00:00 | fail            | 45.57                | 4.68           | 24.19                        |\n",
        "| 7           | 2020-04-08 00:00:00 | success         | 24.45                | 12.69          | 15.91                        |\n",
        "| 8           | 2020-04-23 00:00:00 | success         | 48.22                | 11.2           | 48.82                        |\n",
        "| 9           | 2020-05-08 00:00:00 | success         | 56.63                | 4.04           | 16.08                        |\n",
        "| 10          | 2020-05-23 00:00:00 | fail            | 19.03                | 16.65          | 11.22                        |\n",
        "| 11          | 2020-06-07 00:00:00 | fail            | 81                   | 6.56           | 26.6                         |\n",
        "| 12          | 2020-06-22 00:00:00 | fail            | 21.32                | 8.86           | 28.57                        |\n",
        "| 13          | 2020-07-07 00:00:00 | fail            | 14.74                | 17.76          | 19.33                        |\n",
        "| 14          | 2020-07-22 00:00:00 | success         | 66.73                | 13.68          | 14.07                        |\n",
        "| 15          | 2020-08-06 00:00:00 | success         | 32.98                | 16.17          | 25.34                        |\n",
        "| 16          | 2020-08-21 00:00:00 | success         | 46.49                | 1.84           | 41.9                         |\n",
        "| 17          | 2020-09-05 00:00:00 | fail            | 45.98                | 12.2           | 2.46                         |\n",
        "| 18          | 2020-09-20 00:00:00 | success         | 3.14                 | 24.8           | 36.6                         |\n",
        "| 19          | 2020-10-05 00:00:00 | success         | 75.33                | 23.04          | 29.99                        |\n",
        "| 20          | 2020-10-20 00:00:00 | success         | 53.76                | 22.94          | 18.74                        |\n",
        "\n",
        "---\n",
        "\n",
        "### Explanation of the Query\n",
        "\n",
        "1. **`monthly_aggregates` CTE**:\n",
        "   Aggregates the data by extracting year and month from `request_date`. It sums `distance_to_travel` and `monetary_cost` as `total_distance` and `total_cost`.\n",
        "\n",
        "2. **`distance_per_dollar` CTE**:\n",
        "   Calculates the monthly metric `distance_per_dollar = total_distance / total_cost`.\n",
        "\n",
        "3. **`naive_forecast` CTE**:\n",
        "   Uses the `LAG()` window function to get the previous monthâ€™s `distance_per_dollar` as the forecast for the current month.\n",
        "\n",
        "4. **Final Select Statement**:\n",
        "   Filters out the first month (as it has no prior month), computes the squared error between actual and forecast values, and then calculates the RMSE using `SQRT(AVG(...))`.\n",
        "   The final result is rounded to **2 decimal places**.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "YUJFdYItqkib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Q25. Google (Medium Level)\n",
        "\n",
        "---\n",
        "\n",
        "**You are analyzing a social network dataset at Google. Your task is to find mutual friends between two users, *Karl* and *Hans*.**\n",
        "\n",
        "> *There is only one user named Karl and one named Hans in the dataset.*\n",
        "\n",
        "---\n",
        "\n",
        "### âœ¨ Output should contain:\n",
        "\n",
        "* `user_id`\n",
        "* `user_name` (of the mutual friends)\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”— Understanding how to join tables in SQL is essential for effective data analysis; mastering this concept allows you to combine related data seamlessly. Give it a try! ðŸ‘‡\n",
        "\n",
        "---\n",
        "\n",
        "### Schema and Dataset\n",
        "\n",
        "**`users` Table**\n",
        "\n",
        "| user\\_id | user\\_name |\n",
        "| -------- | ---------- |\n",
        "| 1        | Karl       |\n",
        "| 2        | Hans       |\n",
        "| 3        | Emma       |\n",
        "| 4        | Emma       |\n",
        "| 5        | Mike       |\n",
        "| 6        | Lucas      |\n",
        "| 7        | Sarah      |\n",
        "| 8        | Lucas      |\n",
        "| 9        | Anna       |\n",
        "| 10       | John       |\n",
        "\n",
        "**`friends` Table**\n",
        "\n",
        "| user\\_id | friend\\_id |\n",
        "| -------- | ---------- |\n",
        "| 1        | 3          |\n",
        "| 1        | 5          |\n",
        "| 2        | 3          |\n",
        "| 2        | 4          |\n",
        "| 3        | 1          |\n",
        "| 3        | 2          |\n",
        "| 3        | 6          |\n",
        "| 4        | 7          |\n",
        "| 5        | 8          |\n",
        "| 6        | 9          |\n",
        "| 7        | 10         |\n",
        "| 8        | 6          |\n",
        "| 9        | 10         |\n",
        "| 10       | 7          |\n",
        "| 10       | 9          |\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  Explanation of the Query:\n",
        "\n",
        "1. **CTE `karl_friends` and `hans_friends`**:\n",
        "   Retrieve all friends for Karl (`user_id = 1`) and Hans (`user_id = 2`) from the `friends` table.\n",
        "\n",
        "2. **Main Query**:\n",
        "   Finds the intersection (mutual friends) between Karlâ€™s and Hansâ€™s friends using an `INNER JOIN` on `friend_id`.\n",
        "\n",
        "3. **Final Join with `users` Table**:\n",
        "   Retrieves the `user_name` and `user_id` of the mutual friends using a `JOIN` with the `users` table.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Dptcpmw13i3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Q26. Amazon (Hard Level)\n",
        "\n",
        "---\n",
        "\n",
        "**Given a table `sf_transactions` of purchases by date, calculate the month-over-month percentage change in revenue.**\n",
        "\n",
        "> *Output should include the year-month date (YYYY-MM) and percentage change, rounded to two decimal places.*\n",
        "\n",
        "---\n",
        "\n",
        "###  Output Columns:\n",
        "\n",
        "* `year_month` (formatted as `YYYY-MM`)\n",
        "* `percentage_change` (rounded to 2 decimal places)\n",
        "\n",
        "> The percentage change is calculated as:\n",
        "> `((this monthâ€™s revenue - last monthâ€™s revenue) / last monthâ€™s revenue) * 100`\n",
        "> *Note: First month will have `NULL` since thereâ€™s no prior month to compare.*\n",
        "\n",
        "---\n",
        "\n",
        "###  Schema and Dataset\n",
        "\n",
        "**`sf_transactions` Table**\n",
        "\n",
        "| id | created\\_at         | value  | purchase\\_id |\n",
        "| -- | ------------------- | ------ | ------------ |\n",
        "| 1  | 2019-01-01 00:00:00 | 172692 | 43           |\n",
        "| 2  | 2019-01-05 00:00:00 | 177194 | 36           |\n",
        "| 3  | 2019-01-09 00:00:00 | 109513 | 30           |\n",
        "| 4  | 2019-01-13 00:00:00 | 164911 | 30           |\n",
        "| 5  | 2019-01-17 00:00:00 | 198872 | 39           |\n",
        "| 6  | 2019-01-21 00:00:00 | 184853 | 31           |\n",
        "| 7  | 2019-01-25 00:00:00 | 186817 | 26           |\n",
        "| 8  | 2019-01-29 00:00:00 | 137784 | 22           |\n",
        "| 9  | 2019-02-02 00:00:00 | 140032 | 25           |\n",
        "| 10 | 2019-02-06 00:00:00 | 116948 | 43           |\n",
        "| 11 | 2019-02-10 00:00:00 | 162515 | 25           |\n",
        "| 12 | 2019-02-14 00:00:00 | 114256 | 12           |\n",
        "| 13 | 2019-02-18 00:00:00 | 197465 | 48           |\n",
        "| 14 | 2019-02-22 00:00:00 | 120741 | 20           |\n",
        "| 15 | 2019-02-26 00:00:00 | 100074 | 49           |\n",
        "| 16 | 2019-03-02 00:00:00 | 157548 | 19           |\n",
        "| 17 | 2019-03-06 00:00:00 | 105506 | 16           |\n",
        "| 18 | 2019-03-10 00:00:00 | 189351 | 46           |\n",
        "| 19 | 2019-03-14 00:00:00 | 191231 | 29           |\n",
        "| 20 | 2019-03-18 00:00:00 | 120575 | 44           |\n",
        "| 21 | 2019-03-22 00:00:00 | 151688 | 47           |\n",
        "| 22 | 2019-03-26 00:00:00 | 102327 | 18           |\n",
        "| 23 | 2019-03-30 00:00:00 | 156147 | 25           |\n",
        "\n",
        "---\n",
        "\n",
        "###  Explanation of the Query:\n",
        "\n",
        "1. **CTE `monthly_revenue`**\n",
        "   Groups data by month (`FORMAT(created_at, 'yyyy-MM')`) and computes `SUM(value)` as total monthly revenue.\n",
        "\n",
        "2. **CTE `revenue_with_lag`**\n",
        "   Uses `LAG()` to bring in previous monthâ€™s revenue for comparison.\n",
        "\n",
        "3. **Final SELECT**\n",
        "   Calculates percent change:\n",
        "   `((current - previous) / previous) * 100`, rounded with `ROUND()`.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "CIcRgq3K4J4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-Cd1NWDxray9"
      }
    }
  ]
}